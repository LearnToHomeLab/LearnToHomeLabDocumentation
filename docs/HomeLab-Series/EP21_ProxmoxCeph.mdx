---
sidebar_position: 24
---
import WarningBox from '/src/components/HomepageFeatures/WarningBox';
import InfoBox from '/src/components/HomepageFeatures/InfoBox';
import DocImage from '/src/components/HomepageFeatures/DocImage';

# How to Setup Ceph on Proxmox and Perform Live VM Migration

In this video, we will cover how to perform live VM migration between Proxmox nodes in a cluster using Ceph as the distributed data storage.

<div style={{
  display: 'flex',
  justifyContent: 'center',
  alignItems: 'center',
  height: '100%',
}}>
  <iframe
    width="560"
    height="315"
    src="https://www.youtube.com/embed/gg_6yAt661g?si=8cPM1Sxq5vvEpBYX"
    frameBorder="0"
    allow="accelerometer; autoplay; clipboard-write; encrypted-media; gyroscope; picture-in-picture"
    allowFullScreen
  ></iframe>
</div>

## Setting up a Ceph Storage Pool

Steps to create a Ceph cluster:

1. Have dedicated storage for your Ceph pool (SSD, HDD). It CANNOT be the same disk you already use because it will be wiped.

Ensure you have an extra SSD or HDD installed for your Ceph pool.

<DocImage folder="images/EP21_ProxmoxCeph" name="Still 2025-02-18 105326_1.3.1.png" alt="Ensure dedicated storage device for Ceph pool" />

<WarningBox emoji="⚠️" title="Important">
  The following commands will need to be done on EVERY node (cluster size dependent).
</WarningBox>

Click your node, go to Ceph, and click "Install Ceph."

<DocImage folder="images/EP21_ProxmoxCeph" name="Still 2025-02-18 105326_1.3.2.png" alt="Installing Ceph on node" />

Use Reef (version 18.2). Make sure you install the latest version available at install time. Select the No-Subscription option because we do not pay for Proxmox.

<DocImage folder="images/EP21_ProxmoxCeph" name="Still 2025-02-18 105326_1.3.3.png" alt="Ceph Reef version install" />

On the next screen, press enter to confirm and wait for the install to complete.

<DocImage folder="images/EP21_ProxmoxCeph" name="Still 2025-02-18 105326_1.3.4.png" alt="Ceph installation progress" />

When the blue Next button is enabled, click Next.

<DocImage folder="images/EP21_ProxmoxCeph" name="Still 2025-02-18 105326_1.4.1.png" alt="Ceph install next button" />

Your install should automatically select the network matching that of your node. Confirm and click Next.

<DocImage folder="images/EP21_ProxmoxCeph" name="Still 2025-02-18 105326_1.4.2.png" alt="Ceph network settings" />

### Create the Storage Pool

Click your node name, go to Disks, select your dedicated storage device, and click "Wipe Disk."

<DocImage folder="images/EP21_ProxmoxCeph" name="Still 2025-02-18 105326_1.5.1.png" alt="Wipe dedicated storage device" />

Return to the Ceph menu dropdown near the node name, select OSD, select the wiped device, and click Create.

<DocImage folder="images/EP21_ProxmoxCeph" name="Still 2025-02-18 105326_1.5.2.png" alt="Create OSD from wiped disk" />

Click Pools below the OSD menu, then click Create in the top left.

Set the pool size according to your number of nodes/OSD drives. For example, with two nodes and two drives, set pool size to two. Leave PG autoscaler mode on.

<DocImage folder="images/EP21_ProxmoxCeph" name="Still 2025-02-18 105326_1.6.1.png" alt="Create pool with size setting" />

## How to Perform Live Migrations of VMs

To live migrate a VM, its storage must reside on the Ceph pool.

When creating a VM, on the Disks page select your Ceph pool as the storage option.

<DocImage folder="images/EP21_ProxmoxCeph" name="Still 2025-02-18 105326_1.7.1.png" alt="Selecting Ceph pool for VM disk" />

<InfoBox emoji="ℹ️" title="Note">
  You can edit existing VMs to move their storage to the Ceph pool to enable live migration.
</InfoBox>

### Removing Local Disk for Live Migration

Live migration will NOT work if a CD/DVD device is attached. This device is only needed for initial OS installation.

Steps:
1. Power off your VM.  
2. Select the CD/DVD device under Hardware.  
3. Click Remove.  
4. Power on the VM again.

<DocImage folder="images/EP21_ProxmoxCeph" name="Still 2025-02-18 105326_1.7.2.png" alt="Removing CD/DVD device from VM" />

***Example of live migration failing with a local CD/DVD attached:***

<DocImage folder="images/EP21_ProxmoxCeph" name="Still 2025-02-18 105326_1.7.3.png" alt="Live migration failure example" />

***Example with CD/DVD removed:***

<DocImage folder="images/EP21_ProxmoxCeph" name="Still 2025-02-18 105326_1.7.4.png" alt="Live migration possible after removing CD/DVD" />

Now, with the CD/DVD removed and VM running, select the target node to migrate to and click Migrate.

<DocImage folder="images/EP21_ProxmoxCeph" name="Still 2025-02-18 105326_1.7.6.png" alt="Migrate VM to target node" />

***Live feed of the migration process:***

<DocImage folder="images/EP21_ProxmoxCeph" name="Still 2025-02-18 105326_1.7.7.png" alt="Live migration progress feed" />

After migration, the VM runs on the new node with nearly zero downtime.

<DocImage folder="images/EP21_ProxmoxCeph" name="Still 2025-02-18 105326_1.7.8.png" alt="VM successfully migrated" />

You can now move all VMs away from a node to perform maintenance without downtime or service disruption.

<DocImage folder="images/EP21_ProxmoxCeph" name="Still 2025-02-18 105326_1.7.9.png" alt="Maintenance without downtime" />

## Conclusion

This video covered:
1. How to wipe drives.  
2. How to create a Ceph Pool.  
3. How to live migrate VMs.  
4. How to update other systems without disrupting user availability.

## Follow Us on Social Media

[YouTube](https://www.youtube.com/@learntohomelab)  
[Discord](https://discord.gg/6MsHSJWZpH)  
[Patreon](https://www.patreon.com/c/learntohomelab)  
[Reddit](https://www.reddit.com/r/learntohomelab/)  
[Rumble](https://rumble.com/c/c-7585051)
